%!TEX program = xelatex

\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[UTF8]{ctex}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{multirow}
\newtheorem{definition}{定义}
\newtheorem{theorem}{定理}
\usepackage{subcaption}
\usepackage[export]{adjustbox}

\setCJKmainfont{SimSun}[AutoFakeBold=2.5,ItalicFont=KaiTi]%
\setCJKsansfont{SimHei}[AutoFakeBold=2.5]%
\setCJKmonofont{FangSong}%


\title{《最优化方法》上机作业3：最小二乘方法的数值实验}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  张旻昊 Minhao Zhang 2101213233 \\
  前沿交叉学科研究院 Academy for Advanced Interdisciplinary Studies\\
  北京大学 Peking University\\
  颐和园路5号，海淀，北京 Yiheyuan Rd. $5^{th}$, Haidian, Beijing\\
  \texttt{minhaozhang@pku.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}
%\begin{CJK*}{UTF8}{gbsn}

\maketitle

\begin{abstract}
  本实验实现了小剩余量和大剩余量的Gauss-Newton方法、单折和双折Dogleg方法、ODR方法，并在Osborne数据拟合问题上进行了数值实验，对比分析了上述算法的表现。
  
\end{abstract}

\section{实验设定}
\subsection{目标函数选取}
本实验考虑数据拟合类问题，即存在多个样本点$<t_i, y_i>$，其对应的剩余函数为$r_i(x) = y_i-f(t_i;x)$，我们需要确定模型的参数$x$使得所有样本点的剩余量最小。具体地，对于最小二乘型算法，它最小化所有剩余函数的平方和；对于ODR型算法，则优化考虑$t_i$误差的剩余函数及误差的平方和（换言之，最小化所有样本点到拟合函数的垂直距离平方和）。

为进行实验，本文采用Osborne提出的数据拟合测试问题，如图\ref{fig:osborne}所示，它具有n=11个参数，m=65对拟合数据，同时给出了指数型拟合函数。下文将不同的数据拟合算法应用于本测试问题中以比较表现。

\begin{figure}[h]
  \centering
  \includegraphics[width=.8\linewidth]{pics/osborne.jpg}
  \caption{Osborne数据拟合测试函数。}
  \label{fig:osborne}
\end{figure}

\subsection{实现的算法}

\subsubsection{大剩余量与小剩余量Gauss-Newton方法}
为比较小剩余与大剩余量方法，本文基于Gauss-Newton方法，每步利用$(J_k^t J_k + B_k) d_k = -J_k^t r_k$求参数更新方向，然后利用Fibonacci法进行精确线搜索确定更新量。对于小剩余量方法，令$B_k=0$；对于大剩余量方法，通过$B_{k+1} = B_k + \frac{(y_k-B_k s_k)y_k^t+y_k{(y_k-B_k s_k)}^t}{y_k^t s_k} - \frac{{(y_k-B_k s_k)}^t s_k}{{y_k^t s_k}^2}y_k y_k^t$逐步计算二阶项，注意这一公式与教材相同。

\subsubsection{单折与双折Dogleg方法}
Dogleg方法是求解信赖域子问题的一种方法，首先我们利用信赖域算法求解最小二乘问题，每步更新信赖域半径$\Delta_k$，Dogleg方法则是一种在信赖域内寻找$d_k$的方法。具体地，单折Dogleg利用$d_k^{SD}$和$d_k^{GN}$选择$d_k$，双折Dogleg则进一步引入$\eta_k d_k^{GN}$优化$d_k$的选择。二者的具体算法与教材中相同，这里不再赘述。

\subsubsection{ODR方法}
ODR方法是对最小二乘问题框架的一种扩展，在原始最小二乘问题中，我们最小化剩余函数的平方和，即样本点到拟合曲线的纵轴距离的平方和；对于ODR方法，它考虑样本点可能存在横轴误差，因此最小化样本点到拟合曲线的垂直距离平方和：
\[ \min_{x,\delta} \sum\limits_{i=1}^n \{{(y_i-f(t_i+\delta_i;x))}^2 + \delta_i^2\} \]
注意这一问题可进一步被化为具有2n个剩余函数的最小二乘问题，其中$r_i(x,\delta)=y_i-f(t_i+\delta_i;x), 0\leq i\leq n; r_i(x,\delta)=\delta_i, n+1\leq i\leq 2n$。这样一来，即可使用最小二乘算法求解ODR问题，具体地，Boggs1985使用信赖域方法求解，可以利用Dogleg方法求解信赖域子问题。此外，还需注意将ODR化为最小二乘问题后其Jacobi矩阵的右上角、右下角子矩阵均为对角矩阵，其左下角子矩阵为0，因此可以提高其计算效率，本文所进行的实验也加入了这一效率优化。


\section{实验结果}
本节比较上述算法的数值表现，我将分别对比分析单折与双折Dogleg方法的表现、大/小剩余量Gauss-Newton方法的表现、ODR方法的表现。在实验中，默认初始值使用Osborne问题提供的初始参数（见图\ref{fig:osborne}），Gauss-Newton类方法均使用20次迭代的Fibonacci搜索进行精确线搜索，ODR方法初始化误差项$\delta=0$。在上述默认设定下，所有方法的表现如表\ref{tab:overall}所示。

\begin{table*}[h]
  \centering
  \begin{tabular}{l c c c c c c}
    \toprule
    \bfseries Methods & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries Jeval\\
    \cmidrule(lr){1-7}
    Single-Dogleg & 0.020069 & $<\epsilon$ & 12.8715 & 12 & 25 & 12 \\
    Double-Dogleg & 0.020069 & $<\epsilon$ & 9.7732 & 12 & 25 & 12 \\
    \cmidrule(lr){1-7}
    GN-Small & 0.020069 & 6.14e-5 & 12.0391 & 13 & 346 & 13 \\
    GN-Large & 0.020069 & $<\epsilon$ & 26.5562 & 28 & 859 & 28 \\
    \cmidrule(lr){1-7}
    ODR-Single & 0.016710 & $<\epsilon$ & 15.6048 & 16 & 33 & 16 \\
    ODR-Double & 0.016710 & $<\epsilon$ & 14.5138 & 16 & 33 & 16 \\
    \bottomrule
  \end{tabular}
  \caption{各类方法在Osborne问题上的数值表现。表中GN-Small指小剩余量的Gauss-Newton方法，GN-Large指大剩余量的Gauss-Newton方法，ODR-Single指基于单折Dogleg求解ODR问题的算法，ODR-Double指基于双折Dogleg求解ODR问题的算法；Time指CPU时间，niter为迭代轮数，feval分别Jeval分别指剩余函数（向量）和最小二乘问题的Jacobi矩阵的计算次数，$||g^*||$为所有最小二乘目标函数在收敛点的导数范数。$\epsilon=1e-6$。}
  \label{tab:overall}
\end{table*}

\subsection{单折与双折Dogleg对比}

首先比较两种Dogleg方法的表现，如表\ref{tab:overall}的第1和2行所示，可做如下分析：
\begin{itemize}
  \item 二者均能较好地收敛至Osborne问题的全局最优点，与之对应的是线搜索型算法（表\ref{tab:overall}的第3和4行）收敛的精度不及这二者或速度显著慢于这二者，这展现了信赖域型方法在最小二乘问题上的有效性。
  \item 在解信赖域子问题时，双折Dogleg法的表现优于单折Dogleg，在收敛到同样精度的前提下，双折所用的时间显著低于单折，这说明更精细的$d_k$选择逻辑帮助算法更好地收敛，展现了双折Dogleg改进的有效性。
\end{itemize}

\subsection{大/小剩余量方法对比}
如表\ref{tab:overall}的第3和4行所示，基于Gauss-Newton方法的大、小剩余量方法表现不如基于Dogleg的信赖域型方法。此外，在默认初始点进行数值实验时，小剩余量方法很难达到所需的精度，但大剩余量方法需要更多的迭代次数进而耗时显著更高，因此表\ref{tab:overall}很难直接反映大、小剩余量方法的优劣。

值得注意的是，大剩余量方法的提出是针对$x$距离$x^*$较远或剩余函数非线性程度较高的情况，这提示我们或许在默认初始点增加不同幅度的噪声多次运行算法（进而拟合剩余量逐步增大的情况），有助于更好地考察大剩余量相比小剩余量方法的不同。因此，我们在图\ref{fig:osborne}所给的初始值基础上，从不同标准差、期望为0的正态分布中采样随机噪声加入初始值，分别运行大、小剩余量算法，所得结果如表\ref{tab:noise}所示。

\begin{table*}[h]
  \centering
  \begin{tabular}{c l c c c c c c}
    \toprule
    \bfseries Noise Scale & \bfseries Methods & \boldmath{$f(x^*)$} & \boldmath $||g(x^*)||$ & \bfseries Time(s) & \bfseries niter & \bfseries feval & \bfseries Jeval\\
    \cmidrule(lr){1-8}
    \multirow{2}{*}{0.0} &
    GN-Small & 0.020069 & 6.14e-5 & 12.0391 & 13 & 346 & 13 \\
    & GN-Large & 0.020069 & $<\epsilon$ & 26.5562 & 28 & 859 & 28 \\
    \cmidrule(lr){1-8}
    \multirow{2}{*}{0.2} &
    GN-Small & 0.020069 & 7.35e-6 & 54.7881 & 34 & 899 & 34 \\
    & GN-Large & 0.020069 & $<\epsilon$ & 45.3116 & 27 & 832 & 27 \\
    \cmidrule(lr){1-8}
    \multirow{2}{*}{0.4} &
    GN-Small & 0.020069 & 1.56e-5 & 105.186 & 57 & 1528 & 57 \\
    & GN-Large & 0.020069 & $<\epsilon$ & 55.1325 & 30 & 935 & 30 \\
    \bottomrule
  \end{tabular}
  \caption{在初始点上施加不同大小的随机噪声后，大、小剩余量方法的数值表现对比。其中所有噪声均取自期望为0的正态分布，Noise Scale代表正态分布的标准差（特别的，Scale=0时代表不增加噪声），GN-Small/Large分别指小、大剩余量的Gauss-Newton方法，Time指CPU时间，niter为迭代轮数，feval分别Jeval分别指剩余函数（向量）和最小二乘问题的Jacobi矩阵的计算次数，$||g^*||$为所有最小二乘目标函数在收敛点的导数范数。$\epsilon=1e-6$。}
  \label{tab:noise}
\end{table*}

根据表\ref{tab:noise}可做如下分析：
\begin{itemize}
  \item 对应Gauss-Newton方法，噪声的确会显著影响其收敛速度，且随着噪声幅度的增大收敛变得越来越慢，这一方面表明了线搜索型方法解最小二乘问题对初始值的敏感性，另一方面也提示我们寻找一个好的初始值是实现高效求解的第一步。
  \item 在不存在噪声时，小剩余量方法显著优于大剩余量方法，但当噪声逐渐增加，后者逐渐超越前者，且后者相对前者的优势随噪声的增大而扩大。由于噪声的大小即一定程度上反映了初始点上的剩余量大小，这一结论可以说明：当初始剩余量更大时，考虑$S_k$项的大剩余量方法具有相对优势；但当初始剩余量很小时，小剩余量GN方法本身已经是较好的近似，可以实现相对高效的收敛；这一结论与大剩余量方法提出的初衷相吻合。
\end{itemize}

\subsection{ODR方法的分析}
由于ODR方法可化为特殊的最小二乘问题并通过信赖域方法求解，我们分别基于单折与双折Dogleg方法进行数值实验，结果如表\ref{tab:overall}的第5、6行所示。对此我们做出如下讨论：
\begin{itemize}
  \item 即便加入了计算Jacobi矩阵时的性能优化，ODR方法由于更多更复杂的剩余函数形式，在耗时和所需迭代次数上仍高于相同设定下的原始信赖域方法。
  \item 在ODR方法内部，使用双折Dogleg方法仍相较单折Dogleg具有一定的效率优势，但这种优势并不显著，实际应用中二者的选择或许不会对数值表现有决定性影响。
  \item 虽然ODR方法引入的复杂性一定程度上降低了运行效率，但它引入的额外参数使得模型能更好地拟合数据样本，这体现在其收敛的目标函数值$f(x^*)$低于同样设定下信赖域方法求解原始最小二乘问题的目标函数值。在这种意义上，ODR方法可以对样本数据有更强的拟合能力。
\end{itemize}

\section{总结}
本文讨论最小二乘问题上的多种最优化算法，包括大、小剩余量的Gauss-Newton方法，基于单折与双折Dogleg的信赖与方法，ODR方法。本文通过Osborne数据拟合问题比较了上述方法的数值表现并进行分析，通过引入额外噪声的实验进一步分析大剩余方法的意义。


%end{CJK*}
\end{document}
